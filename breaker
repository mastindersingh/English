
Analytics and Reporting Layer (MicroStrategy & Tableau)
Once data is consolidated in WXPS’s databases, Wells Fargo leverages enterprise business intelligence (BI) tools to visualize and report on this information. The two primary tools in use (as per the context diagram) are MicroStrategy and Tableau. These tools interface with the WXPS Gold DB (and possibly directly with other DBs for certain use cases) to provide dashboards, reports, and analytical capabilities to various stakeholders:
* MicroStrategy: MicroStrategy is an enterprise BI platform known for its scalable reporting, dashboards, and distribution capabilities
*  . It offers interactive dashboards, scorecards, ad-hoc querying, and scheduled report distribution – all important for an organization of Wells Fargo’s size . In WXPS, MicroStrategy likely provides standardized operational reports and executive dashboards. For example, there might be MicroStrategy reports showing enterprise-wide metrics such as: number of endpoints by business unit, percentage of devices on the latest Windows 11 version, top 10 software installed, compliance scorecards (e.g., encryption or antivirus deployment rates). Because MicroStrategy can be fully web-based and can email reports, it’s useful for scheduled reporting to management. It may also allow drill-down dashboards for technology managers to slice data (by region, by division, etc.). In the WXPS functional diagram, MicroStrategy is depicted connecting to the TMT SOR, implying it queries the SQL databases directly to fetch data for its reports. MicroStrategy’s enterprise features (like fine-grained security, ability to handle large datasets, and integration with authentication systems) align well with the needs of WXPS to deliver sensitive internal data to the right people.
* Tableau: Tableau is a leading visual analytics and data visualization platform . It enables users to create interactive and intuitive charts, graphs, and dashboards from data. Wells Fargo appears to use Tableau alongside MicroStrategy, likely for more ad-hoc analysis and data exploration by analysts or for visually rich dashboards to support decision-making. Tableau’s strength is in its user-friendly drag-and-drop interface and the ability to quickly uncover patterns in data through visuals. In the context of WXPS, a Tableau dashboard might be used by an “end user computing” team to monitor real-time stats (like a live dashboard of new incidents coming in vs devices deployed), or by a technology executive to explore data on employee technology utilization. For example, a Tableau visualization could show a map of branch offices with counts of devices and their health status, or a trend line of print jobs over time as a measure of digital adoption. Tableau is described as an end-to-end analytics and BI platform that helps everyone make data-driven decisions , so giving teams access to WXPS data via Tableau fosters a data-driven approach to improving workplace tech. Analysts could merge WXPS data with other data (maybe performance data or service desk data) in Tableau for deeper insight.
Both MicroStrategy and Tableau would be configured to connect to the WXPS SQL Server (likely via ODBC/JDBC or native connectors). They would use the Gold DB schemas for most reporting since that has the business-friendly refined data. In some cases, for advanced users, they might query the Vault DB if historical raw analysis is needed, but typically the curated layer suffices. Security and Access: It’s worth noting that access to data via these BI tools would be restricted – not everyone can see all data. WXPS likely defines roles (for example, only HR or managers can see individual employee names associated with devices, etc.). MicroStrategy and Tableau both allow integration with Active Directory groups for security. So WXPS data access in these tools is probably governed by the user’s AD credentials (with SiteMinder SSO passing those through, as we’ll discuss next). This ensures that only authorized teams (like IT support staff, tech managers, or risk teams) can view sensitive information about endpoints and users. In summary, the analytics layer of WXPS turns the raw data into actionable intelligence. Through scheduled reports in MicroStrategy, WXPS can inform leadership about compliance and inventory status. Through interactive Tableau dashboards, WXPS empowers analysts to identify trends (e.g., a spike in software installs or an area with many outdated devices) and thereby drive decisions (like where to invest in upgrades or training). This combination addresses both static reporting needs and dynamic analysis needs of the enterprise.
User Access and System Integration
WXPS not only integrates data behind the scenes but also integrates with enterprise access management and application hosting standards. The functional diagram snippet indicates components like Azure IIS, ASP .NET MVC, SiteMinder, Channel Secure, and AD which relate to how users or applications interact with WXPS in a secure way. Here’s how WXPS fits into the enterprise IT ecosystem from a user access and app perspective:
* Intranet Web User Interface: There is likely an internal web application (or a set of web pages on Wells Fargo’s intranet) that serves as a portal for WXPS. This could be used by IT support or engineering teams to query the WXPS data on demand or to manage certain configurations. It might be an ASP.NET MVC application (as hinted by the diagram) hosted on an Azure IIS environment (possibly an internal cloud or Azure Stack). This web UI would allow authorized users to search for an employee and see their devices, or search for a device ID and see its details, etc., without having to use the raw database. It makes WXPS data more accessible in daily operations (e.g., a helpdesk technician can quickly look up a user’s machine info when they call for support).
* SiteMinder Integration: CA SiteMinder (now Broadcom SSO) is an enterprise single sign-on solution. In WXPS, SiteMinder acts as the authentication gatekeeper for the intranet web UI (and possibly for MicroStrategy as well). When a user tries to access the WXPS portal or BI reports, SiteMinder verifies their identity (typically against AD) and their authorization via policies. SiteMinder is widely used to enforce login and session management for intranet apps, so WXPS leverages this to ensure only authenticated Wells Fargo employees can reach the WXPS application.
* Channel Secure and LDAP (AD): The mention of “Channel Secure” suggests an internal secure middleware or gateway that interfaces with LDAP/AD for validating sessions or queries. Possibly, Channel Secure is an in-house secure proxy that the web app uses to query AD (for user info) or to ensure the connection is within allowed network zones. It might also refer to Wells Fargo’s multi-factor or network trust solution for intranet apps. In any case, Active Directory (LDAP) is the source of truth for user credentials and roles; WXPS integrates with it for authentication and role-based access control. For example, only members of a certain AD group (like “IT-Asset-Management-Team”) might be allowed to access certain WXPS data.
* Hosting & Middleware: The architecture diagram shows Azure IIS and ASP.NET MVC which implies the WXPS web components are built on Microsoft web technologies and possibly hosted in Azure or a private cloud environment. This aligns with Wells Fargo’s adoption of cloud for some internal apps (without exposing them publicly). By using standard web frameworks, WXPS can integrate with enterprise logging, monitoring, and deployment pipelines as any other internal web application.
All these pieces ensure WXPS is not a standalone silo but part of the enterprise fabric. It adheres to corporate security (SSO, AD integration), uses standard scheduling and ETL tools as discussed, and standard database technology (SQL Server) – making it maintainable and secure. From a user perspective, how WXPS is accessed might look like: an IT user goes to an internal portal URL (e.g., a Confluence page or custom web portal for WXPS), SiteMinder seamlessly logs them in using their Windows credentials, and then they can either run reports (which open MicroStrategy/Tableau in the browser, also integrated with SSO) or use a search interface to retrieve specific data on demand. The experience would be unified thanks to the SSO integration.
Use Cases and Functions Supported by WXPS
Bringing together the above components, WXPS supports several important use cases in managing the enterprise workplace environment. Some of the key functions and workflows enabled by WXPS at Wells Fargo include:
* Comprehensive Asset Inventory: WXPS provides a one-stop inventory of all end-user devices in the company – desktops, laptops, thin clients, mobile devices (if integrated), printers (via Pharos), etc. This inventory is enriched with user information from HR and AD. The benefit is any authorized person can query “who has device X” or “what devices does employee Y have” and get an instant answer, rather than consulting multiple systems. The inventory includes hardware details (from SCCM/JAMF), software installed, peripheral assignments, and more. Such a consolidated inventory is crucial for IT asset management and lifecycle planning.
* Software Inventory and License Compliance: By aggregating software installation data from SCCM (Windows) and JAMF (Mac), WXPS can produce enterprise-wide software inventory reports. This helps track how many instances of a software are installed and where, aiding in license compliance and optimization. For example, if Microsoft Office or Adobe licenses are limited, WXPS data can show actual deployment counts. The data from Symantec (Altiris) also contributes by verifying installed software lists. With reporting tools, the software inventory can be sliced by department or version to identify who needs upgrades or where old versions are still in use.
* Employee Endpoint Analytics: WXPS can correlate multiple data points about an employee’s “digital experience.” For instance, linking HR data (tenure, department) with IT data (devices, software, support tickets, printing behavior, etc.) allows analysis of how technology is being used by different segments of employees. Endpoint analytics may include metrics like: average number of devices per employee, average login times or issues (if data from login systems or APM tools were integrated), frequency of contacting support, or software usage patterns. Over time, WXPS can help identify trends such as “Employees in department A have more computing issues than department B” or “A particular laptop model is associated with more complaints.” These insights drive improvements (perhaps upgrading hardware or providing targeted training).
* Proactive IT Support & Refresh Cycles: Thanks to the Vault historical data, WXPS can support forecasting and proactive maintenance. For example, by tracking device ages and performance incidents, WXPS might highlight devices that are over 4 years old and due for replacement. It could also reveal if certain software is approaching end-of-life on many machines, prompting an enterprise upgrade. Support teams can use WXPS dashboards to catch anomalies – e.g., a spike in printer errors in a region (from Pharos data) could alert them to check those devices.
* Reporting and Metrics for Management: WXPS supplies key performance indicators (KPIs) and operational metrics to management. Some typical reports could be:
    * Compliance Reports: e.g., percentage of devices with required security software installed (Symantec data), or patch compliance rate by division.
    * Utilization Reports: e.g., number of active vs inactive devices (to reclaim unused assets), application usage (if integrated with usage metrics or just presence/absence on devices).
    * Service Delivery Reports: e.g., mean time to fulfill a technology request (by correlating PAC2000 request dates to deployment dates captured in inventory), or volume of IT tickets by category (if Remedy data is integrated).
    * Cost and Resource Reports: Combining Controllers DataMart info with inventory to show asset costs by department, or printing costs saved by certain initiatives, etc.
* These reports can be scheduled (using MicroStrategy) to go out monthly to stakeholders, ensuring everyone has visibility into the state of the enterprise’s workplace technology.
* Integration with Risk and Compliance: In a bank, regulatory compliance is critical. WXPS’s accurate system-of-record data can feed into risk assessments. For example, if an audit asks “provide a list of all computers that have access to sensitive data and whether they have disk encryption,” WXPS can answer that by cross-referencing device inventory with Symantec encryption status and HR roles. The platform can thus respond to regulatory or audit inquiries quickly with confidence in the data’s accuracy (since it’s consolidated and cleaned continuously).
In summary, WXPS supports both operational activities (helping IT support and engineering teams day-to-day) and strategic planning (giving management data to make decisions on technology investments and policies). It improves efficiency by automating data gathering (no more spreadsheets from different teams – WXPS has it all), and it improves the employee experience indirectly by enabling IT to be more proactive and informed.
Conclusion
WXPS (Workplace Experience Platform Services) at Wells Fargo is a sophisticated integration of data engineering and IT service management. It centralizes information from many enterprise systems into a unified platform that acts as the system of record for employee-related technology assets and usage. Through components like the TMT SOR databases (Realtime, Vault, Gold) and integration tools (AutoSys, SSIS, NDM), WXPS ensures data from HR, device management (Windows SCCM, JAMF for Mac, etc.), network, security, and support systems is continually collected and reconciled. The platform then delivers this information back to the business via intuitive reports and dashboards in MicroStrategy and Tableau, as well as via internal web interfaces, all secured by enterprise single sign-on. By doing so, WXPS provides visibility and insight into the workplace technology environment: from tracking every laptop and software install, to analyzing how employees interact with technology on a daily basis. This empowers technology teams to maintain compliance, optimize software licenses, plan hardware refreshes, and swiftly address issues – ultimately driving a better experience for team members using the bank’s technology and increasing productivity. In a modern banking enterprise, such a platform is indispensable for digital workplace transformation, ensuring that the right data gets to the right people for informed decision-making about employee technology needs. Wells Fargo’s WXPS exemplifies how integrating data across siloed systems can unlock value and serve as a foundation for a more efficient and employee-centric IT ecosystem. Sources: The description above is based on the WXPS architecture as outlined in internal context diagrams and industry knowledge of the tools involved. Key references include definitions of PeopleSoft HCM
,



* JAMF Pro (Casper Suite): JAMF Pro (formerly called the Casper Suite) is the Apple device management platform used for macOS computers. It provides inventory data and management capabilities for Macs . WXPS pulls data from JAMF to cover all Apple endpoints – this includes Mac hardware details, installed applications on Macs, macOS versions, compliance statuses, etc. By integrating JAMF data, WXPS achieves a full inventory of enterprise devices across both Windows and macOS environments.
* Symantec Management/Endpoint: Wells Fargo uses Symantec endpoint management/endpoint protection solutions on client machines. Symantec’s client management suite runs an agent on each computer that automatically gathers inventory data (hardware, OS, installed software) whenever the agent is installed . Additionally, Symantec Endpoint Protection provides security status (antivirus, encryption, etc.). WXPS ingests data from Symantec systems to incorporate security and compliance information about endpoints (e.g., antivirus status, encryption status, etc.). This helps in endpoint analytics, such as identifying machines without updated antivirus or seeing patterns in security incidents.
* HPDM (HP Device Manager) & WDM (Wyse Device Manager): These systems manage thin clients or special devices. HP Device Manager is used for HP thin clients, and Wyse Device Manager for Dell Wyse thin clients. They provide inventory and status of these devices. Integrating this data into WXPS allows the platform to account for all endpoint types (not just full PCs, but also thin clients used in branches or call centers).
* Controllers DataMart: This likely refers to an internal finance or asset “controllers” data mart. It might contain information about asset ownership, cost centers, depreciation, or financial tracking of equipment. By feeding this into WXPS, the platform can align technical inventory with financial records (e.g., confirming if a device is capitalized, who owns the budget for it, etc.). It could also mean data from “IT controllers” regarding compliance. (In summary, it supplies supplementary data about devices or users from a finance/control perspective.)
* PAC2000 / Remedy: PAC2000 is the Wells Fargo internal branding for the BMC Remedy IT Service Management system  . This system holds IT tickets, incident and request data. WXPS pulling from Remedy means it can incorporate service records – for example, when a device was last serviced, if a user has open IT issues, or records of software requests. This can enrich the WXPS data with lifecycle events .
* Pharos Blueprint (Print Management): Pharos Blueprint is an on-premises print management solution for large enterprises . It tracks printing activity (who printed, how many pages, which printer) and manages print devices. WXPS integrating Blueprint data provides insight into print usage patterns per user or device – an aspect of employee technology experience. For example, it can report how much an employee prints or identify costly printing behaviors, contributing to the overall workplace technology analytics.

Each of these sources typically provides data to WXPS on a scheduled basis (some daily, some near-real-time, depending on the nature of data and system capabilities). Next, we discuss how WXPS ingests and processes this data.

ETL Utility Layer – Data Ingestion and Integration
To absorb the data from the various sources and load it into the central repository, WXPS employs an ETL (Extract, Transform, Load) pipeline along with scheduling and data transfer utilities. In the Wells Fargo WXPS architecture, the Utility layer is explicitly represented  and includes components such as AutoSys, SSIS/OLEDB, and NDM. Each plays a role in moving and transforming data:
* AutoSys (Workload Automation): AutoSys is an enterprise job scheduler/workload automation system used to run and monitor jobs across multiple platforms. It allows scheduling of ETL jobs, scripts, and other tasks in a controlled manner . In WXPS, AutoSys is used to schedule regular data imports from the source systems.an AutoSys job might run nightly to fetch the latest HR data or to trigger a SSIS package that processes SCCM inventory. AutoSys provides robust scheduling (with dependencies, conditions, and monitoring), ensuring the WXPS data pipeline runs reliably and on time . Essentially, AutoSys orchestrates the entire data load sequence for WXPS (e.g., “run job to pull PeopleSoft data, then after success, run job to update database, then run reporting refresh jobs”, etc.).
* SSIS (SQL Server Integration Services) with OLEDB: SSIS is Microsoft’s ETL tool, part of SQL Server, for building data integration and transformation solutions .WXPS uses SSIS packages to perform the data extraction and transformation logic. For instance, an SSIS package might connect to an Oracle database (PeopleSoft) via OLE DB, extract employee records, transform them to fit the WXPS schema, and load them into the SQL Server database. Another SSIS package might take a CSV export from JAMF  and import it into the database. SSIS can handle a wide variety of source formats (XML, flat files, relational databases, etc.) and load into SQL Server In the WXPS utility layer, “SSIS – OLEDB” indicates that SSIS is leveraging OLE DB connectors to talk to source systems (OLE DB is a Microsoft data access technology for connecting to external databases). This combination forms the ETL engine that cleanses, maps, and loads incoming data into the WXPS data stores.
* NDM (Network Data Mover): NDM refers to Connect a similar secure file transfer mechanism often called “Network Data Mover.” It’s used for reliable, batch file transfers between systems. In WXPS, NDM likely is used when source systems deliver data as files. For example, the PeopleSoft HR system might drop a nightly extract file of active employees, or the LAN central system could output a CSV of network ports. NDM jobs (also scheduled by AutoSys) would move these files from the source system’s environment into the WXPS staging area for SSIS to consume. NDM ensures secure and checkpointed transfer of large data files across the network 
Integration Workflow: The utility layer works as follows – AutoSys triggers SSIS packages at predefined times. These packages use OLE DB or other connectors to pull data from source systems or accept data files that have been transferred via NDM. The data is then transformed to match the WXPS data model. For example, converting fields, merging records, removing duplicates, etc., and finally loading into the target SQL databases. Logging and error handling are part of the SSIS packages, and AutoSys monitors for job success/failure (with alerts if something fails). This framework is robust and typical for enterprise data warehousing: scheduling + ETL + file transfer working in concert. It allows WXPS to aggregate fresh data on a daily (or more frequent) basis, keeping the central repository updated. By using enterprise-standard tools (AutoSys for scheduling, SSIS for ETL), WXPS integrates well into the bank’s IT infrastructure – these tools are widely supported and can interface with many technologies. The outcome of this ETL process populates the core databases of WXPS


Central Data Repository – TMT SOR and SQL Databases
At the heart of WXPS is the Team Member Technology System of Record (TMT SOR) – the central data repository that stores all the integrated information. Physically, this repository is implemented in Microsoft SQL Server and is broken into multiple databases for different purposes. The key databases as noted in the architecture are: Realtime DB, Vault DB, and Gold DB. Together, these form a layered data warehouse within WXPS:
* Realtime DB: This is likely a database that holds the most current snapshot of data. It might be where the latest data from each source is first loaded (possibly a staging or operational data store). The name “Realtime” suggests it could be updated frequently or even in near-real-time for certain sources. For example, if some data sources send updates more often than daily, they may be reflected here. The Realtime DB could be structured to allow quick updates/inserts of incoming records (perhaps mirroring the structure of source data with minimal transformation for speed). It provides WXPS users a view of the latest known state of all endpoints and assets at any given time.
* 
* Vault DB: The Vault database acts as a historical vault of all collected data. In data warehousing terms, this aligns with a Data Vault modeling approach, which emphasizes long-term historical storage of data from multiple sources with full traceability . The Vault DB likely stores all versions of records (with effective dates or audit timestamps), enabling WXPS to retain history (e.g., to answer questions like “What software did this device have last quarter?” or “Which device did employee  use before their current one?”). It is essentially an archive of raw facts – “all the data, all of the time” in raw form – analogous to a “single version of the facts” data vault concept . The Vault DB gives resilience to change (since no data is thrown away) and supports auditing and trend analysis over time.
* Gold DB: The Gold database contains the curated, refined data ready for consumption and reporting. In a multi-layer data architecture, “gold” typically refers to the final enriched layer that is cleansed, business-rule-applied, and optimized for analysis . The Gold DB in WXPS likely combines data from Realtime/Vault, applies business logic (for example, linking employee records to devices, or marking the primary device per user, etc.), and organizes it into a schema suitable for reporting . It may aggregate or summarize certain information to speed up queries (for instance, total number of devices per department, or compliance percentage metrics). This is the presentation layer of WXPS data – when Tableau or MicroStrategy queries WXPS, they primarily hit the Gold DB to get clean and business-ready data.
This multi-tiered database approach ensures separation of concerns: the Realtime DB handles raw updates, the Vault DB preserves history, and the Gold DB presents finalized data. It’s comparable to the industry practice of Bronze/Silver/Gold layers in data lakes (raw → refined → curated)

, adapted here in a SQL warehouse context. By structuring the data this way, WXPS can meet both operational needs (quick updates and current state) and analytical needs (historical analysis and easy querying). 
Data Model and SOR Function: As an SOR for team member technology, the TMT SOR data model likely has tables for Employees, Devices, Applications (software titles), perhaps relationships linking them, and tables for various metrics (print jobs, support tickets, etc.). For example, an Employee table (from HR data) links to one or more Device records (from SCCM/JAMF) by an employee ID. A Device record links to Software Inventory records listing installed software. There might be a Software Catalog reference (to normalize software names/versions). There could be a Location or Network Node table fed by LAN Central, linked to devices (to know where a device is connected). The Vault DB would store changes in these links over time (so if a user gets a new laptop, the old device record is marked retired and a new one linked, with history preserved). The “System of Record” aspect means WXPS’s TMT SOR becomes the authoritative source for any queries about “who has what technology” within the bank. Prior to WXPS, that info might have been siloed (HR knows who the employee is, SCCM knows their laptop, but only by joining do we see the full picture). Now TMT SOR has it all in one place. A system of record stores valuable data about an organizational process in a reliable way


 – here the process is asset management of employee tech and the employee experience metrics around it. WXPS likely also implements data quality checks in this layer. For instance, if HR says an employee is terminated, but device inventory still shows they have an active device, WXPS can flag that for reconciliation. Or if two sources conflict on a device’s location, rules in the Gold layer decide which to trust. In summary, the central WXPS databases (Realtime, Vault, Gold) collectively enable WXPS to answer both up-to-the-minute questions and long-term trend questions about employee technology assets, all while maintaining a robust audit trail.






Python Packaging Project Setup Guide –
wellsfargo_1pii_kpi_automation
Introduction
This document provides guidelines for setting up a Python packaging project in a Wells Fargo internal context, using the wellsfargo_1pii_kpi_automation project as an example. It covers two standard methods of packaging a Python project:
• Setuptools with setup.cfg + pyproject.toml – the traditional packaging approach using Setuptools.
• Poetry with pyproject.toml – a modern all-in-one approach using Poetry.
We compare these methods, highlight why mixing both systems in one project is discouraged, and present recommended configurations for each. Explanations are provided for each configuration section (metadata, options, dependencies, build system, etc.), along with notes on using Artifactory as a private PyPI repository. Finally, we outline best practices regarding versioning (semantic versioning), CI/CD pipeline compatibility, and dependency locking for reproducible environments.
Comparison of Packaging Methods
Both Setuptools and Poetry achieve the goal of making your Python project installable as a package, but they differ in configuration style and tooling. The table below summarizes key differences:
     Aspect Setuptools (setup.cfg)
Poetry (pyproject.toml)
    Configuration Files
setup.cfg (for package metadata) + minimal
pyproject.toml (PEP 517/518 build config). May also include setup.py (often minimal or
none).
Single pyproject.toml file containing all metadata under [tool.poetry] and dependencies under
[tool.poetry.dependencies] . No separate setup.cfg or setup.py .
      Declared in
Metadata [options] Definition setup.cfg
[metadata] and sections of
(or in setup.py
Declared in the [tool.poetry] table in TOML format. Poetry requires fields like name, version, description, authors, etc.
1 .
   script). Uses an INI-like syntax.
1
  Aspect Setuptools (setup.cfg)
Poetry (pyproject.toml)
         Dependencies Definition
Listed as install_requires in setup.cfg 's [options] section
(or requirements.txt for dev/ test). No built-in lock file; rely on pip to resolve at install time.
Listed in [tool.poetry.dependencies] (and
dev in [tool.poetry.dev- dependencies] ). Poetry generates a
poetry.lock lockfile to pin exact versions for reproducibility 2 3 .
       Build System Backend
Setuptools (specified via [build- system] in pyproject). Example: setuptools.build_meta with requirements setuptools and
wheel 4 .
Poetry's core backend ( poetry-core ) (via [build-system] in pyproject). Example: build-backend = "poetry.core.masonry.api" with requirement poetry-core>=1.0.0 .
                Tooling & Workflow
Use pip or build to create distributions (wheel, sdist). E.g.
pip install . or python -m build uses Setuptools. Dependencies installed via pip, possibly using a
requirements.txt for dev.
Use Poetry CLI for most tasks ( poetry install , poetry add ,
poetry build , poetry publish ). Alternatively, pip can build using the Poetry backend since poetry-core is installed, but typically Poetry manages env and build.
        Managed outside of packaging (e.g., Dev separate dev-requirements.txt
Dependencies or using pip-tools). Not included in the package metadata.
Declared in [tool.poetry.dev- dependencies] (or optional groups) in the same pyproject. Not included in built distribution, but captured in lockfile for dev environment.
       No native lockfile. Recommended to Lockfile for use pip freeze or pip-
Reproducibility compile to create a requirements lock file for exact versions in CI.
Uses poetry.lock to lock dependency versions. This should be committed to version control for reproducible installs across environments.
    Defined under
Entry Points / [options.entry_points] in
Scripts setup.cfg (e.g. console_scripts for CLI).
Defined under
[tool.poetry.scripts] in pyproject.toml to create console script
entry points.
     Repository Sources
By default uses PyPI or an index URL configured in pip. Private index (Artifactory) is configured via pip settings or environment.
Can specify additional package indexes in pyproject via [[tool.poetry.source]] (or use poetry config to add repositories). Supports authenticating to private repos (Artifactory) via config 5 .
 2

 Aspect Setuptools (setup.cfg) Poetry (pyproject.toml)
  CI/CD Integration
Requires installing build dependencies (setuptools, wheel). Use pip to install and test. Might require separate steps for resolving dev requirements.
Poetry can manage a venv in CI or you can install poetry to use it. Alternatively, in CI one can use pip install . which will invoke Poetry’s build backend. Poetry’s lockfile ensures consistent deps in CI.
  Use Case Considerations
Well-suited for projects that prefer minimal dependencies (leveraging tools that come with pip). Many existing projects and tools expect setup.cfg.
Good for projects that want an all-in-one tool for dependency management and packaging. Facilitates easier dependency management and environment isolation, at the cost of introducing the Poetry tool.
Table: High-level comparison of Setuptools vs Poetry packaging approaches.
In summary, Setuptools relies on standard Python packaging tooling and separate configuration files, whereas Poetry provides a more integrated approach with a single config and built-in dependency management. Mixing the two systems in one project is strongly discouraged, as it can lead to conflicting configurations and tools ignoring each other’s settings (detailed below).
Packaging with Setuptools ( setup.cfg and minimal pyproject.toml )
In the Setuptools-based approach, we use the setup.cfg file to declaratively specify package metadata and options, and a minimal pyproject.toml to indicate the build system requirements. This is the “traditional” method aligning with standard setuptools usage and PEP 517/518 compliance.
Project Structure: It is recommended to use a structured layout for the project. For example:
wellsfargo_1pii_kpi_automation/ ├── src/
     │
│
│
├── tests/
│ └── ... (your test modules) ├── setup.cfg
└── wellsfargo_1pii_kpi_automation/ ├── __init__.py
└── ... (your Python modules)
├── pyproject.toml ├── README.md
└── LICENSE
In this structure, the package code resides in src/wellsfargo_1pii_kpi_automation . The setup.cfg and pyproject.toml live at the project root.
   3

 Example: setup.cfg (Setuptools Configuration)
[metadata]
  name = wellsfargo_1pii_kpi_automation
  version = 0.1.0
  description = Automation of 1PII KPI calculations and reporting
  author = Wells Fargo (Internal Team)
  license = Proprietary
  classifiers =
      Programming Language :: Python :: 3.9
      Programming Language :: Python :: 3.10
      Programming Language :: Python :: 3.11
      License :: Other/Proprietary
      Operating System :: OS Independent
[options]
  packages = find:
  package_dir =
      = src
  python_requires = >=3.9
  install_requires =
      pandas>=1.5,<2.0
      openpyxl>=3.0
  include_package_data = True
  [options.packages.find]
where = src
  ; Optional: include console script entry point
  [options.entry_points]
  console_scripts =
      kpi-auto = wellsfargo_1pii_kpi_automation.cli:main
Explanation:
• [metadata] : Contains package metadata:
• name : Distribution name of the package (should be unique).
• version : Package version (here 0.1.0 following semantic versioning).
• description : Short summary of the package.
• author : Name of the author or team (for internal projects, this could be a team or department).
• license : License identifier (e.g., "Proprietary" for internal code, or an OSI license if applicable).
• classifiers : A list of Trove classifiers for package indexing. These include programming language version support and license, etc.
• [options] : Main package configuration:
         4

 • •
• •
•
• •
•
packages = find: tells setuptools to automatically find sub-packages. We then specify package_dir to point to the src directory.
package_dir mapping '= src' indicates that our package modules are under the src/
directory.
python_requires : Specifies the Python version compatibility (here, Python 3.9+). install_requires : Lists runtime dependencies (with version constraints). In this example, the
project depends on pandas (>=1.5,<2.0) and openpyxl (>=3.0). These will be installed when someone installs the package.
include_package_data : If True , includes files specified in MANIFEST.in or data files inside packages.
[options.packages.find] : Additional options for package finding:
where = src directssetuptoolstolookinsidethe src directoryforpackages.Thispairswiththe
packages = find: directive above. (If the code was not in a src folder, this could be omitted or adjusted.)
[options.entry_points] (optional): Defines console scripts or other entry points. In this example, we define a console script named kpi-auto that points to a main function in a wellsfargo_1pii_kpi_automation/cli.py module. This means after installation, running kpi-auto will execute that main() function. (This section is only needed if the project provides
command-line tools.)
                  With setup.cfg configured, we also create a minimal pyproject.toml to specify Setuptools as the build backend.
Example: pyproject.toml (for Setuptools build system)
  [build-system]
  requires = [
      "setuptools>=61.0",
      "wheel"
  ]
  build-backend = "setuptools.build_meta"
Explanation:
   • •
•
[build-system] : This section is mandated by PEP 517/518 to declare the build system.
requires : Lists the packages needed to build the project. Here we require setuptools (version 61.0 or higher, to support setup.cfg declarative config) and wheel (so that wheels can be built) 4 . This ensures that when tools like pip build the project, they first install these build requirements.
build-backend : Specifies the build backend. "setuptools.build_meta" is the standard backend for Setuptools builds. This tells pip or build frontends to use Setuptools to build the package (using the config in setup.cfg ).
       5

 This minimal pyproject.toml is enough for Setuptools. No [tool.poetry] section is present, since we are not using Poetry in this approach. Tools like pip will read this file, install Setuptools and wheel, then execute the build.
Usage and Workflow (Setuptools method)
• Building the package: You can build distribution artifacts using the standard PyPA tool build or pip. For example:
• python -m build will create a source distribution ( .tar.gz ) and a wheel ( .whl ) in the dist/ directory.
• Alternatively, pip install . will build and install the package in one step.
• Installing in development: For dev work, you might use pip install -e . (editable install) to install the package in development mode. Setuptools will use the above configuration to install the
package in a way that edits in code are reflected.
• Managing dev dependencies: Since Setuptools does not handle dev dependencies in setup.cfg ,
you might maintain a requirements-dev.txt or use a tool like pip-tools to pin dev requirements. These could include testing frameworks (e.g., pytest ) or linters which are not needed in production installation.
• Publishing to Artifactory: To publish the built package to an internal PyPI (Artifactory), you can use tools like Twine. For example:
• Build the distributions as above, then run twine upload --repository-url <Artifactory URL> -u <user> -p <api_key> dist/* . The <Artifactory URL> would be the endpoint of your PyPI repository in Artifactory (e.g., https://<artifactory>/artifactory/api/pypi/ <repo>/ ).
• Ensure your project’s version is updated (in setup.cfg ) before each release, following semantic versioning.
Packaging with Poetry ( pyproject.toml only)
In the Poetry-based approach, all project configuration is consolidated in a single pyproject.toml file. Poetry manages package metadata, dependencies, and build configuration from this file, and it also maintains a lock file for precise dependency versions. No setup.cfg or setup.py is needed when using Poetry.
Start by ensuring Poetry is initialized for your project (e.g., via poetry init or poetry new ). This will create a basic pyproject.toml . You can then edit it as needed. For our example project, the pyproject might look like:
  [tool.poetry]
  name = "wellsfargo_1pii_kpi_automation"
  version = "0.1.0"
  description = "Automation of 1PII KPI calculations and reporting"
  authors = ["Wells Fargo Team <[email protected]>"]
  license = "Proprietary"
  readme = "README.md"
  # If relevant, specify the package as a package (for src layout)
                          6

    packages = [
      { include = "wellsfargo_1pii_kpi_automation", from = "src" }
]
  [tool.poetry.dependencies]
  python = ">=3.9,<4.0"
  pandas = "^1.5.0"
  openpyxl = "^3.0.0"
  # Other dependencies can be listed with version constraints (Caret (^) means
  compatible releases)
  [tool.poetry.dev-dependencies]
  pytest = "^7.0"
  flake8 = "^6.0"
  # Dev/test dependencies not included in the final package
  [tool.poetry.scripts]
  kpi-auto = "wellsfargo_1pii_kpi_automation.cli:main"
  # Optional: If using an internal PyPI repository in Artifactory for some
  dependencies
  # [[tool.poetry.source]]
  # name = "artifactory-internal"
  # url = "https://<your_artifactory_domain>/artifactory/api/pypi/<repo-name>/
  simple/"
  # default = true  # or priority = "primary"
  [build-system]
  requires = ["poetry-core>=1.5.0"]
  build-backend = "poetry.core.masonry.api"
Explanation:
• •
•
• • •
• •
  6 7 : name : Package name (same as the distribution name, and usually the Python import package).
[tool.poetry] : This table holds the core metadata for the project (managed by Poetry)
Poetry assumes you have a package directory matching this name in your project 8 . version : Package version, ideally following semantic versioning (e.g., MAJOR.MINOR.PATCH).
Poetry requires the version to be PEP 440 compliant 9 .
description : A short description of the project.
authors : List of authors with name and email.
license : License identifier (if applicable). For internal projects, this could be “Proprietary” or
omitted.
readme : Path to README file (to include as long description).
packages : (Optional) If using a src/ layout, this specifies which packages to include and from which path. In the example, we include the wellsfargo_1pii_kpi_automation package from
       7

the src directory. (If not using src layout, Poetry can auto-detect the package, but it's clearer to specify.)
• [tool.poetry.dependencies] : Lists all runtime dependencies of the project: • python : Specifies the Python versions supported (here we require >=3.9, <4.0).
• Other lines (e.g., pandas , openpyxl ) specify dependencies and versions. Poetry allows version constraints like caret ( ^1.5.0 ) meaning "compatible with 1.5.0" (roughly >=1.5.0,<2.0.0 in SemVer terms). These will be used by Poetry to resolve and lock exact versions in the poetry.lock file.
• [tool.poetry.dev-dependencies] : Lists development dependencies (not needed at runtime for users of the package, but used in development and testing).
• E.g., pytest and flake8 are included for testing and linting. These are only installed when you do poetry install in a development context (unless you add --without dev ). They will not be installed when someone installs the package from PyPI/Artifactory, because they are not part of the published package requirements.
(Note: In Poetry 1.2+, you can also define groups for dev dependencies, but the [tool.poetry.dev- dependencies] section remains a simple way to list them.)
• [tool.poetry.scripts] : Defines command-line entry points (similar to console_scripts in setuptools).
• In this example, kpi-auto is a script name that will invoke wellsfargo_1pii_kpi_automation.cli:main . This allows users to run kpi-auto after
installing the package, same effect as described in the setuptools example.
• [[tool.poetry.source]] (Optional, shown commented out): This section can be repeated to add external package indexes (e.g., an internal Artifactory repository):
• name : Identifier for the repository (e.g., "artifactory-internal").
• url : The base URL of the simple index API of Artifactory PyPI repository.
• default = true (or priority = "primary" in newer Poetry) indicates this is the primary
source for packages (which disables the implicit default PyPI). Alternatively, one can set priority = "supplemental" to use Artifactory as a secondary source 10 11 .
Use this if some dependencies are only available internally, or if you want to ensure Poetry installs from ArtifactoryinsteadofpublicPyPI.Donotincludecredentialsinthisfile.Instead,use poetry config to set up authentication (for example, using the CLI commands poetry config repositories.<name> <url> and poetry config http-basic.<name> <username> <password> to store credentials securely on the machine running Poetry 5 ).
• [build-system] : Specifies Poetry’s build backend:
                          8

  • •
requires : We require poetry-core (the lightweight build backend for Poetry). Version
>=1.5.0 is indicated in this example; adjust to the Poetry core version appropriate (Poetry will usually set this for you on poetry new ).
build-backend : Set to "poetry.core.masonry.api" , which is the entry point for Poetry’s build process. This allows PEP 517 build frontends (like pip) to build the project even without the Poetry CLI installed, by using the poetry-core library.
    Once this pyproject.toml is configured, all packaging information is in place. Poetry will also generate or update poetry.lock whenever dependencies are added or updated.
Usage and Workflow (Poetry method)
• Installing dependencies: Run poetry install to create a virtual environment and install all dependencies (including dev if not using --no-dev ). This uses the lock file to ensure exact versions.
• Adding dependencies: Use poetry add <package> or poetry add --dev <package> which updates the pyproject and lockfile.
• Building the package: Use poetry build to generate the wheel and sdist (output in the dist/ directory).
• Publishing: Use poetry publish to upload to PyPI or a configured repository. For Artifactory, ensure you configured the repository in Poetry ( poetry config repositories.<repo> as above, and credentials). Then poetry publish -r <repo> will upload the package to Artifactory.
• CI/CD: In a CI environment, you can either:
• Install Poetry on the runner and run poetry install && poetry run pytest (for tests) and
poetry build / poetry publish .
• Or use PEP 517 via pip: e.g., pip install . will use the pyproject.toml to install build
requirements (poetry-core) and install the package. (However, to install dev dependencies in CI, using
the Poetry CLI is easier.)
• The poetry.lock file should be committed to your git repository. This ensures that all developers
and CI pipelines use the same resolved versions of dependencies, improving reproducibility 12 13 . Avoid Mixing Setuptools and Poetry in One Project
It is not recommended to mix the Setuptools (setup.cfg) and Poetry approaches in the same project. Each approach expects to manage the project configuration independently, and using both can cause conflicts and confusion:
• If a pyproject.toml contains a [tool.poetry] section, Poetry will treat the project as a Poetry-managed project and ignore other configuration sources. For example, Poetry will ignore dependencies listed in setup.cfg ; it reads only pyproject for that information 14 .
• You would end up maintaining duplicate metadata (version, description, dependencies) in two places (setup.cfg and pyproject), which is error-prone and defeats the purpose of a single source of truth.
• Build tools may become confused about which backend to use. A pyproject with Poetry config and
also a build-system pointing to setuptools could lead to unintended behavior. In fact, as one Poetry maintainer noted: “you cannot mix Poetry’s way of defining package metadata/dependency with those of setuptools... you have to decide how you want to manage your project: Poetry or setuptools. Both doesn’t
work.” 15 . In other words, use one method or the other, not both.
                   9

Recommendation: Choose one packaging method at project initialization. If you are migrating from one to the other, remove the obsolete configuration files. For instance, if converting an existing setuptools project toPoetry,remove setup.cfg (andany setup.py)oncepyproject.tomlissetupwithPoetry.Conversely, if extracting a Poetry project to standard setuptools, remove the [tool.poetry] sections from pyproject.toml.
By keeping the configuration single-sourced, you avoid contradictory definitions and simplify the build process.
Best Practices and Additional Notes
Finally, regardless of which packaging approach you use, consider the following best practices for an internal project:
Semantic Versioning and Release Management
Adoptsemanticversioningforyourproject’s version.ThismeansusingaMAJOR.MINOR.PATCHformat: - Increment the MAJOR version for incompatible API changes, - Increment the MINOR version for adding functionality in a backwards-compatible manner, - Increment the PATCH version for backwards-compatible bug fixes.
For example, going from 1.2.3 to 1.3.0 indicates a new feature (non-breaking), whereas 2.0.0 would indicate breaking changes. This convention helps consumers of your package (even if internal) understand the impact of an upgrade. Note that Python packaging requires versions to conform to PEP 440 syntax, which is largely compatible with semantic versioning (Poetry, for instance, enforces PEP 440 compliance 9 ). Stick to numeric versions and use pre-release tags (e.g., 1.0.0b1 for a beta,
1.0.0.dev1 for a development preview) if needed for testing.
Ensure the version in your configuration (be it setup.cfg or pyproject.toml ) is bumped appropriately on each release. This version is what will be published to Artifactory and is used by pip for dependency resolution.
Dependency Management and Locking
For reproducible builds and environments, it’s important to lock dependencies: - In Poetry, the poetry.lock file serves this purpose by pinning exact versions of all dependencies (including transitive dependencies). Commit this lock file to version control, and use it in CI to install deps. This guarantees that all developers and CI use the same dependency versions 12 13 . - In the setuptools approach, consider generating a requirements.txt (or similar) with pinned versions for the development environment or deployment. Tools like pip freeze can capture the environment, or better, use pip-compile (from pip-tools) to derive a locked requirements file from your install_requires . This isn’t used by pip when installing your library (which will use the version ranges in install_requires ), but it’s useful for your own testing and CI to ensure consistency. - Avoid overly broad dependency specifications. For example, pin a minimum version that you have tested, and if possible upper-bound if you know newer major releases might break compatibility. (Poetry’s caret ( ^ ) and tilde ( ~ ) version operators can help constrain ranges in a maintainable way.) - Periodically review and update dependencies to incorporate security patches and
                  10

  improvements, and update the lock file (Poetry’s poetry update , or updating the install_requires and regen the requirements.txt for setuptools).
CI/CD Integration
Integrate the packaging and distribution steps into your CI/CD pipeline: - Testing: Ensure your CI runs tests (e.g., via pytest) in an environment where your package is installed just as it would be for a user. For setuptools, this could mean doing pip install . (or pip install dist/yourpkg.whl ) before running tests. For Poetry, you can use poetry install && poetry run pytest . - Building: Have the CI produce build artifacts on each release (wheels and source distributions). The python -m build tool can be invoked in a pipeline for setuptools projects. For Poetry, poetry build does the same. - Publishing to Artifactory: Use pipeline steps to publish the built package to Artifactory PyPI. This might involve using an API key or credentials stored in the pipeline. For example, using Twine (for setuptools) or
poetry publish (forPoetry).Makesurecredentialsarenothard-codedinconfigfiles,butratherinjected via environment variables or CI secret management. - Environment Isolation: In CI, use a fresh virtual environment for builds to avoid contamination from previous runs. Poetry automatically creates an isolated venv for you. With pip/setuptools, create a venv or use containerized jobs. - Verification: Optionally, after publishing, the pipeline could verify the package can be installed from Artifactory (perhaps in a staging environment) to catch any issues with missing dependencies or files.
Additional Tips
• Don’t commit build artifacts: Wheel and sdist files in dist/ should be produced by CI or manually for releases, but not kept under version control.
• Documentation: Keep your README and docstrings up-to-date as they can be included in the package metadata (e.g., long_description on PyPI).
• Consistent Tooling: If your team uses both methods across different projects, ensure that each project’s README or docs specify how to build and release it (so a developer knows whether to use Poetry commands or standard pip/setuptools commands). Consistency within a single project is key.
• Artifactory considerations: If using Artifactory, ensure the index URL and credentials are properly configured. For pip/setuptools users, this might be a pip --index-url or pip.ini/pip.conf configuration. For Poetry, use the aforementioned poetry source or global config. The goal is that a developer (or CI) doing poetry install or pip install yourpkg does so seamlessly using the internal repository.
By following these guidelines, the wellsfargo_1pii_kpi_automation project (and others like it) will have a clear, maintainable packaging setup. Whether using Setuptools or Poetry, sticking to one approach and adhering to best practices will make the project easier to build, test, and deploy within Wells Fargo’s systems.
Note: To obtain this document as a Markdown file, you can copy the above content into a text editor and save it with a .md extension. For a Word document, you may convert the Markdown to .docx using a tool (e.g., Pandoc) or paste the content into MS Word (the formatting will render thanks to the structured headings, lists, and code blocks).
               11

1 2 3 6 7 12 13 Dependency Management With Python Poetry – Real Python https://realpython.com/dependency-management-python-poetry/
4 Making Sense of pyproject.toml, setup.cfg, and setup.py - DEV Community https://dev.to/2320sharon/making-sense-of-pyprojecttoml-setupcfg-and-setuppy-2o6m
5 Set Up PyPI with a Poetry Client https://jfrog.com/help/r/jfrog-artifactory-documentation/set-up-pypi-with-a-poetry-client
8 Basic usage | Documentation | Poetry - Python dependency ... https://python-poetry.org/docs/basic-usage/
9 The pyproject.toml file | Documentation | Poetry - Python dependency management and packaging made easy
https://python-poetry.org/docs/pyproject/
10 11 python - How to add a private repository using poetry? - Stack Overflow https://stackoverflow.com/questions/74323364/how-to-add-a-private-repository-using-poetry
14 15 poetry not installing dependencies from install_requires · Issue #4648 · python-poetry/poetry · GitHub
https://github.com/python-poetry/poetry/issues/4648
12



Python Packaging Project Setup Guide – wellsfargo_1pii_kpi_automation
Introduction
This document provides guidelines for setting up a Python packaging project in a Wells Fargo internal context, using the wellsfargo_1pii_kpi_automation project as an example. It covers two standard methods of packaging a Python project:
Setuptools with setup.cfg + pyproject.toml – the traditional packaging approach using Setuptools.
Poetry with pyproject.toml – a modern all-in-one approach using Poetry.
We compare these methods, highlight why mixing both systems in one project is discouraged, and present recommended configurations for each. Explanations are provided for each configuration section (metadata, options, dependencies, build system, etc.), along with notes on using Artifactory as a private PyPI repository. Finally, we outline best practices regarding versioning (semantic versioning), CI/CD pipeline compatibility, and dependency locking for reproducible environments.
Comparison of Packaging Methods
Both Setuptools and Poetry achieve the goal of making your Python project installable as a package, but they differ in configuration style and tooling. The table below summarizes key differences:
Aspect	Setuptools (setup.cfg)	Poetry (pyproject.toml)
Configuration Files	setup.cfg (for package metadata) + minimal pyproject.toml (PEP 517/518 build config). May also include setup.py (often minimal or none).	Single pyproject.toml file containing all metadata under [tool.poetry] and dependencies under [tool.poetry.dependencies]. No separate setup.cfg or setup.py.
Metadata Definition	Declared in [metadata] and [options] sections of setup.cfg (or in setup.py script). Uses an INI-like syntax.	Declared in the [tool.poetry] table in TOML format. Poetry requires fields like name, version, description, authors, etc.
realpython.com
.
Dependencies Definition	Listed as install_requires in setup.cfg's [options] section (or requirements.txt for dev/test). No built-in lock file; rely on pip to resolve at install time.	Listed in [tool.poetry.dependencies] (and dev in [tool.poetry.dev-dependencies]). Poetry generates a poetry.lock lockfile to pin exact versions for reproducibility
realpython.com
realpython.com
.
Build System Backend	Setuptools (specified via [build-system] in pyproject). Example: setuptools.build_meta with requirements setuptools and wheel
dev.to
.	Poetry's core backend (poetry-core) (via [build-system] in pyproject). Example: build-backend = "poetry.core.masonry.api" with requirement poetry-core>=1.0.0.
Tooling & Workflow	Use pip or build to create distributions (wheel, sdist). E.g. pip install . or python -m build uses Setuptools. Dependencies installed via pip, possibly using a requirements.txt for dev.	Use Poetry CLI for most tasks (poetry install, poetry add, poetry build, poetry publish). Alternatively, pip can build using the Poetry backend since poetry-core is installed, but typically Poetry manages env and build.
Dev Dependencies	Managed outside of packaging (e.g., separate dev-requirements.txt or using pip-tools). Not included in the package metadata.	Declared in [tool.poetry.dev-dependencies] (or optional groups) in the same pyproject. Not included in built distribution, but captured in lockfile for dev environment.
Lockfile for Reproducibility	No native lockfile. Recommended to use pip freeze or pip-compile to create a requirements lock file for exact versions in CI.	Uses poetry.lock to lock dependency versions. This should be committed to version control for reproducible installs across environments.
Entry Points / Scripts	Defined under [options.entry_points] in setup.cfg (e.g. console_scripts for CLI).	Defined under [tool.poetry.scripts] in pyproject.toml to create console script entry points.
Repository Sources	By default uses PyPI or an index URL configured in pip. Private index (Artifactory) is configured via pip settings or environment.	Can specify additional package indexes in pyproject via [[tool.poetry.source]] (or use poetry config to add repositories). Supports authenticating to private repos (Artifactory) via config
jfrog.com
.
CI/CD Integration	Requires installing build dependencies (setuptools, wheel). Use pip to install and test. Might require separate steps for resolving dev requirements.	Poetry can manage a venv in CI or you can install poetry to use it. Alternatively, in CI one can use pip install . which will invoke Poetry’s build backend. Poetry’s lockfile ensures consistent deps in CI.
Use Case Considerations	Well-suited for projects that prefer minimal dependencies (leveraging tools that come with pip). Many existing projects and tools expect setup.cfg.	Good for projects that want an all-in-one tool for dependency management and packaging. Facilitates easier dependency management and environment isolation, at the cost of introducing the Poetry tool.
Table: High-level comparison of Setuptools vs Poetry packaging approaches. In summary, Setuptools relies on standard Python packaging tooling and separate configuration files, whereas Poetry provides a more integrated approach with a single config and built-in dependency management. Mixing the two systems in one project is strongly discouraged, as it can lead to conflicting configurations and tools ignoring each other’s settings (detailed below).
Packaging with Setuptools (setup.cfg and minimal pyproject.toml)
In the Setuptools-based approach, we use the setup.cfg file to declaratively specify package metadata and options, and a minimal pyproject.toml to indicate the build system requirements. This is the “traditional” method aligning with standard setuptools usage and PEP 517/518 compliance. Project Structure: It is recommended to use a structured layout for the project. For example:
wellsfargo_1pii_kpi_automation/
├── src/
│   └── wellsfargo_1pii_kpi_automation/
│       ├── __init__.py
│       └── ... (your Python modules)
├── tests/
│   └── ... (your test modules)
├── setup.cfg
├── pyproject.toml
├── README.md
└── LICENSE
In this structure, the package code resides in src/wellsfargo_1pii_kpi_automation. The setup.cfg and pyproject.toml live at the project root.
Example: setup.cfg (Setuptools Configuration)
[metadata]
name = wellsfargo_1pii_kpi_automation
version = 0.1.0
description = Automation of 1PII KPI calculations and reporting
author = Wells Fargo (Internal Team)
license = Proprietary
classifiers =
    Programming Language :: Python :: 3.9
    Programming Language :: Python :: 3.10
    Programming Language :: Python :: 3.11
    License :: Other/Proprietary
    Operating System :: OS Independent

[options]
packages = find:
package_dir =
    = src
python_requires = >=3.9
install_requires =
    pandas>=1.5,<2.0
    openpyxl>=3.0
include_package_data = True

[options.packages.find]
where = src

; Optional: include console script entry point
[options.entry_points]
console_scripts =
    kpi-auto = wellsfargo_1pii_kpi_automation.cli:main
Explanation:
[metadata]: Contains package metadata:
name: Distribution name of the package (should be unique).
version: Package version (here 0.1.0 following semantic versioning).
description: Short summary of the package.
author: Name of the author or team (for internal projects, this could be a team or department).
license: License identifier (e.g., "Proprietary" for internal code, or an OSI license if applicable).
classifiers: A list of Trove classifiers for package indexing. These include programming language version support and license, etc.
[options]: Main package configuration:
packages = find: tells setuptools to automatically find sub-packages. We then specify package_dir to point to the src directory.
package_dir mapping '= src' indicates that our package modules are under the src/ directory.
python_requires: Specifies the Python version compatibility (here, Python 3.9+).
install_requires: Lists runtime dependencies (with version constraints). In this example, the project depends on pandas (>=1.5,<2.0) and openpyxl (>=3.0). These will be installed when someone installs the package.
include_package_data: If True, includes files specified in MANIFEST.in or data files inside packages.
[options.packages.find]: Additional options for package finding:
where = src directs setuptools to look inside the src directory for packages. This pairs with the packages = find: directive above. (If the code was not in a src folder, this could be omitted or adjusted.)
[options.entry_points] (optional): Defines console scripts or other entry points. In this example, we define a console script named kpi-auto that points to a main function in a wellsfargo_1pii_kpi_automation/cli.py module. This means after installation, running kpi-auto will execute that main() function. (This section is only needed if the project provides command-line tools.)
With setup.cfg configured, we also create a minimal pyproject.toml to specify Setuptools as the build backend.
Example: pyproject.toml (for Setuptools build system)
[build-system]
requires = [
    "setuptools>=61.0",
    "wheel"
]
build-backend = "setuptools.build_meta"
Explanation:
[build-system]: This section is mandated by PEP 517/518 to declare the build system.
requires: Lists the packages needed to build the project. Here we require setuptools (version 61.0 or higher, to support setup.cfg declarative config) and wheel (so that wheels can be built)
dev.to
. This ensures that when tools like pip build the project, they first install these build requirements.
build-backend: Specifies the build backend. "setuptools.build_meta" is the standard backend for Setuptools builds. This tells pip or build frontends to use Setuptools to build the package (using the config in setup.cfg).
This minimal pyproject.toml is enough for Setuptools. No [tool.poetry] section is present, since we are not using Poetry in this approach. Tools like pip will read this file, install Setuptools and wheel, then execute the build.
Usage and Workflow (Setuptools method)
Building the package: You can build distribution artifacts using the standard PyPA tool build or pip. For example:
python -m build will create a source distribution (.tar.gz) and a wheel (.whl) in the dist/ directory.
Alternatively, pip install . will build and install the package in one step.
Installing in development: For dev work, you might use pip install -e . (editable install) to install the package in development mode. Setuptools will use the above configuration to install the package in a way that edits in code are reflected.
Managing dev dependencies: Since Setuptools does not handle dev dependencies in setup.cfg, you might maintain a requirements-dev.txt or use a tool like pip-tools to pin dev requirements. These could include testing frameworks (e.g., pytest) or linters which are not needed in production installation.
Publishing to Artifactory: To publish the built package to an internal PyPI (Artifactory), you can use tools like Twine. For example:
Build the distributions as above, then run twine upload --repository-url <Artifactory URL> -u <user> -p <api_key> dist/*. The <Artifactory URL> would be the endpoint of your PyPI repository in Artifactory (e.g., https://<artifactory>/artifactory/api/pypi/<repo>/).
Ensure your project’s version is updated (in setup.cfg) before each release, following semantic versioning.
Packaging with Poetry (pyproject.toml only)
In the Poetry-based approach, all project configuration is consolidated in a single pyproject.toml file. Poetry manages package metadata, dependencies, and build configuration from this file, and it also maintains a lock file for precise dependency versions. No setup.cfg or setup.py is needed when using Poetry. Start by ensuring Poetry is initialized for your project (e.g., via poetry init or poetry new). This will create a basic pyproject.toml. You can then edit it as needed. For our example project, the pyproject might look like:
[tool.poetry]
name = "wellsfargo_1pii_kpi_automation"
version = "0.1.0"
description = "Automation of 1PII KPI calculations and reporting"
authors = ["Wells Fargo Team <[email protected]>"]
license = "Proprietary"
readme = "README.md"
# If relevant, specify the package as a package (for src layout)
packages = [
    { include = "wellsfargo_1pii_kpi_automation", from = "src" }
]

[tool.poetry.dependencies]
python = ">=3.9,<4.0"
pandas = "^1.5.0"
openpyxl = "^3.0.0"
# Other dependencies can be listed with version constraints (Caret (^) means compatible releases)

[tool.poetry.dev-dependencies]
pytest = "^7.0"
flake8 = "^6.0"
# Dev/test dependencies not included in the final package

[tool.poetry.scripts]
kpi-auto = "wellsfargo_1pii_kpi_automation.cli:main"

# Optional: If using an internal PyPI repository in Artifactory for some dependencies
# [[tool.poetry.source]]
# name = "artifactory-internal"
# url = "https://<your_artifactory_domain>/artifactory/api/pypi/<repo-name>/simple/"
# default = true  # or priority = "primary"

[build-system]
requires = ["poetry-core>=1.5.0"]
build-backend = "poetry.core.masonry.api"
Explanation:
[tool.poetry]: This table holds the core metadata for the project (managed by Poetry)
realpython.com
realpython.com
:
name: Package name (same as the distribution name, and usually the Python import package). Poetry assumes you have a package directory matching this name in your project
python-poetry.org
.
version: Package version, ideally following semantic versioning (e.g., MAJOR.MINOR.PATCH). Poetry requires the version to be PEP 440 compliant
python-poetry.org
.
description: A short description of the project.
authors: List of authors with name and email.
license: License identifier (if applicable). For internal projects, this could be “Proprietary” or omitted.
readme: Path to README file (to include as long description).
packages: (Optional) If using a src/ layout, this specifies which packages to include and from which path. In the example, we include the wellsfargo_1pii_kpi_automation package from the src directory. (If not using src layout, Poetry can auto-detect the package, but it's clearer to specify.)
[tool.poetry.dependencies]: Lists all runtime dependencies of the project:
python: Specifies the Python versions supported (here we require >=3.9, <4.0).
Other lines (e.g., pandas, openpyxl) specify dependencies and versions. Poetry allows version constraints like caret (^1.5.0) meaning "compatible with 1.5.0" (roughly >=1.5.0,<2.0.0 in SemVer terms). These will be used by Poetry to resolve and lock exact versions in the poetry.lock file.
[tool.poetry.dev-dependencies]: Lists development dependencies (not needed at runtime for users of the package, but used in development and testing).
E.g., pytest and flake8 are included for testing and linting. These are only installed when you do poetry install in a development context (unless you add --without dev). They will not be installed when someone installs the package from PyPI/Artifactory, because they are not part of the published package requirements.
(Note: In Poetry 1.2+, you can also define groups for dev dependencies, but the [tool.poetry.dev-dependencies] section remains a simple way to list them.)
[tool.poetry.scripts]: Defines command-line entry points (similar to console_scripts in setuptools).
In this example, kpi-auto is a script name that will invoke wellsfargo_1pii_kpi_automation.cli:main. This allows users to run kpi-auto after installing the package, same effect as described in the setuptools example.
[[tool.poetry.source]] (Optional, shown commented out): This section can be repeated to add external package indexes (e.g., an internal Artifactory repository):
name: Identifier for the repository (e.g., "artifactory-internal").
url: The base URL of the simple index API of Artifactory PyPI repository.
default = true (or priority = "primary" in newer Poetry) indicates this is the primary source for packages (which disables the implicit default PyPI). Alternatively, one can set priority = "supplemental" to use Artifactory as a secondary source
stackoverflow.com
stackoverflow.com
.
Use this if some dependencies are only available internally, or if you want to ensure Poetry installs from Artifactory instead of public PyPI. Do not include credentials in this file. Instead, use poetry config to set up authentication (for example, using the CLI commands poetry config repositories.<name> <url> and poetry config http-basic.<name> <username> <password> to store credentials securely on the machine running Poetry
jfrog.com
).
[build-system]: Specifies Poetry’s build backend:
requires: We require poetry-core (the lightweight build backend for Poetry). Version >=1.5.0 is indicated in this example; adjust to the Poetry core version appropriate (Poetry will usually set this for you on poetry new).
build-backend: Set to "poetry.core.masonry.api", which is the entry point for Poetry’s build process. This allows PEP 517 build frontends (like pip) to build the project even without the Poetry CLI installed, by using the poetry-core library.
Once this pyproject.toml is configured, all packaging information is in place. Poetry will also generate or update poetry.lock whenever dependencies are added or updated.
Usage and Workflow (Poetry method)
Installing dependencies: Run poetry install to create a virtual environment and install all dependencies (including dev if not using --no-dev). This uses the lock file to ensure exact versions.
Adding dependencies: Use poetry add <package> or poetry add --dev <package> which updates the pyproject and lockfile.
Building the package: Use poetry build to generate the wheel and sdist (output in the dist/ directory).
Publishing: Use poetry publish to upload to PyPI or a configured repository. For Artifactory, ensure you configured the repository in Poetry (poetry config repositories.<repo> as above, and credentials). Then poetry publish -r <repo> will upload the package to Artifactory.
CI/CD: In a CI environment, you can either:
Install Poetry on the runner and run poetry install && poetry run pytest (for tests) and poetry build/poetry publish.
Or use PEP 517 via pip: e.g., pip install . will use the pyproject.toml to install build requirements (poetry-core) and install the package. (However, to install dev dependencies in CI, using the Poetry CLI is easier.)
The poetry.lock file should be committed to your git repository. This ensures that all developers and CI pipelines use the same resolved versions of dependencies, improving reproducibility
realpython.com
realpython.com
.
Avoid Mixing Setuptools and Poetry in One Project
It is not recommended to mix the Setuptools (setup.cfg) and Poetry approaches in the same project. Each approach expects to manage the project configuration independently, and using both can cause conflicts and confusion:
If a pyproject.toml contains a [tool.poetry] section, Poetry will treat the project as a Poetry-managed project and ignore other configuration sources. For example, Poetry will ignore dependencies listed in setup.cfg; it reads only pyproject for that information
github.com
.
You would end up maintaining duplicate metadata (version, description, dependencies) in two places (setup.cfg and pyproject), which is error-prone and defeats the purpose of a single source of truth.
Build tools may become confused about which backend to use. A pyproject with Poetry config and also a build-system pointing to setuptools could lead to unintended behavior. In fact, as one Poetry maintainer noted: “you cannot mix Poetry’s way of defining package metadata/dependency with those of setuptools… you have to decide how you want to manage your project: Poetry or setuptools. Both doesn’t work.”
github.com
. In other words, use one method or the other, not both.
Recommendation: Choose one packaging method at project initialization. If you are migrating from one to the other, remove the obsolete configuration files. For instance, if converting an existing setuptools project to Poetry, remove setup.cfg (and any setup.py) once pyproject.toml is set up with Poetry. Conversely, if extracting a Poetry project to standard setuptools, remove the [tool.poetry] sections from pyproject.toml. By keeping the configuration single-sourced, you avoid contradictory definitions and simplify the build process.
Best Practices and Additional Notes
Finally, regardless of which packaging approach you use, consider the following best practices for an internal project:
Semantic Versioning and Release Management
Adopt semantic versioning for your project’s version. This means using a MAJOR.MINOR.PATCH format:
Increment the MAJOR version for incompatible API changes,
Increment the MINOR version for adding functionality in a backwards-compatible manner,
Increment the PATCH version for backwards-compatible bug fixes.
For example, going from 1.2.3 to 1.3.0 indicates a new feature (non-breaking), whereas 2.0.0 would indicate breaking changes. This convention helps consumers of your package (even if internal) understand the impact of an upgrade. Note that Python packaging requires versions to conform to PEP 440 syntax, which is largely compatible with semantic versioning (Poetry, for instance, enforces PEP 440 compliance
python-poetry.org
). Stick to numeric versions and use pre-release tags (e.g., 1.0.0b1 for a beta, 1.0.0.dev1 for a development preview) if needed for testing. Ensure the version in your configuration (be it setup.cfg or pyproject.toml) is bumped appropriately on each release. This version is what will be published to Artifactory and is used by pip for dependency resolution.
Dependency Management and Locking
For reproducible builds and environments, it’s important to lock dependencies:
In Poetry, the poetry.lock file serves this purpose by pinning exact versions of all dependencies (including transitive dependencies). Commit this lock file to version control, and use it in CI to install deps. This guarantees that all developers and CI use the same dependency versions
realpython.com
realpython.com
.
In the setuptools approach, consider generating a requirements.txt (or similar) with pinned versions for the development environment or deployment. Tools like pip freeze can capture the environment, or better, use pip-compile (from pip-tools) to derive a locked requirements file from your install_requires. This isn’t used by pip when installing your library (which will use the version ranges in install_requires), but it’s useful for your own testing and CI to ensure consistency.
Avoid overly broad dependency specifications. For example, pin a minimum version that you have tested, and if possible upper-bound if you know newer major releases might break compatibility. (Poetry’s caret (^) and tilde (~) version operators can help constrain ranges in a maintainable way.)
Periodically review and update dependencies to incorporate security patches and improvements, and update the lock file (Poetry’s poetry update, or updating the install_requires and regen the requirements.txt for setuptools).
CI/CD Integration
Integrate the packaging and distribution steps into your CI/CD pipeline:
Testing: Ensure your CI runs tests (e.g., via pytest) in an environment where your package is installed just as it would be for a user. For setuptools, this could mean doing pip install . (or pip install dist/yourpkg.whl) before running tests. For Poetry, you can use poetry install && poetry run pytest.
Building: Have the CI produce build artifacts on each release (wheels and source distributions). The python -m build tool can be invoked in a pipeline for setuptools projects. For Poetry, poetry build does the same.
Publishing to Artifactory: Use pipeline steps to publish the built package to Artifactory PyPI. This might involve using an API key or credentials stored in the pipeline. For example, using Twine (for setuptools) or poetry publish (for Poetry). Make sure credentials are not hard-coded in config files, but rather injected via environment variables or CI secret management.
Environment Isolation: In CI, use a fresh virtual environment for builds to avoid contamination from previous runs. Poetry automatically creates an isolated venv for you. With pip/setuptools, create a venv or use containerized jobs.
Verification: Optionally, after publishing, the pipeline could verify the package can be installed from Artifactory (perhaps in a staging environment) to catch any issues with missing dependencies or files.
Additional Tips
Don’t commit build artifacts: Wheel and sdist files in dist/ should be produced by CI or manually for releases, but not kept under version control.
Documentation: Keep your README and docstrings up-to-date as they can be included in the package metadata (e.g., long_description on PyPI).
Consistent Tooling: If your team uses both methods across different projects, ensure that each project’s README or docs specify how to build and release it (so a developer knows whether to use Poetry commands or standard pip/setuptools commands). Consistency within a single project is key.
Artifactory considerations: If using Artifactory, ensure the index URL and credentials are properly configured. For pip/setuptools users, this might be a pip --index-url or pip.ini/pip.conf configuration. For Poetry, use the aforementioned poetry source or global config. The goal is that a developer (or CI) doing poetry install or pip install yourpkg does so seamlessly using the internal repository.
By following these guidelines, the wellsfargo_1pii_kpi_automation project (and others like it) will have a clear, maintainable packaging setup. Whether using Setuptools or Poetry, sticking to one approach and adhering to best practices will make the project easier to build, test, and deploy within Wells Fargo’s systems. Note: To obtain this document as a Markdown file, you can copy the above content into a text editor and save it with a .md extension. For a Word document, you may convert the Markdown to .docx using a tool (e.g., Pandoc) or paste the content into MS Word (the formatting will render thanks to the structured headings, lists, and code blocks).




# Python Project Packaging Guidelines (Wells Fargo)

## Overview

This document defines the standard packaging approach for Python projects within Wells Fargo using either `setuptools` or `poetry`.

---

## Option 1: Setuptools (`setup.cfg` + `pyproject.toml`)

### `setup.cfg`
```ini
[metadata]
name = wellsfargo_1pii_kpi_automation
version = 1.1.0
description = A Python project for KPI automation

[options]
packages = find:
install_requires =
    jira
    psycopg2-binary
    pymssql
    pytest
    pytest-cov
    coverage
zip_safe = True
include_package_data = True
python_requires = >=3.11

[options.package_data]
* = README.md

[options.packages.find]
where = .
```

### `pyproject.toml`
```toml
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"
```

> ✅ Use this approach when using standard Jenkins or legacy CICD build tools.

---

## Option 2: Poetry-Only (`pyproject.toml`)

```toml
[tool.poetry]
name = "wellsfargo_1pii_kpi_automation"
version = "1.1.0"
description = "A Python project for KPI automation"
authors = ["K154844 <mastinder.singh@wellsfargo.com>"]
readme = "README.md"

packages = [
  { include = "kpi_automation" }
]

[[tool.poetry.source]]
name = "KPI_automation"
url = "https://artifactory-intg-b2.wellsfargo.net/artifactory/api"
priority = "primary"

[tool.poetry.dependencies]
python = ">=3.11,<3.12"
jira = "^3.4.0"
psycopg2-binary = "^2.9.5"
pymssql = "^2.3.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

> ✅ Use this if you manage dependencies with `poetry install`, `poetry lock`, and plan to deploy using Poetry pipelines.

---

## Important Notes

- **Do not mix** `setup.cfg` and `tool.poetry` in the same project.
- Maintain **semantic versioning** for easier artifact traceability.
- Always validate syntax (`toml-lint`, `poetry check`, or `python -m build`).

---

Would you like this Confluence content in downloadable `.md` or `.docx` format?




# Python Project Packaging Guidelines (Wells Fargo)

## Overview

This document defines the standard packaging approach for Python projects within Wells Fargo using either `setuptools` or `poetry`.

---

## Option 1: Setuptools (`setup.cfg` + `pyproject.toml`)

### `setup.cfg`
```ini
[metadata]
name = wellsfargo_1pii_kpi_automation
version = 1.1.0
description = A Python project for KPI automation

[options]
packages = find:
install_requires =
    jira
    psycopg2-binary
    pymssql
    pytest
    pytest-cov
    coverage
zip_safe = True
include_package_data = True
python_requires = >=3.11

[options.package_data]
* = README.md

[options.packages.find]
where = .
```

### `pyproject.toml`
```toml
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"
```

> ✅ Use this approach when using standard Jenkins or legacy CICD build tools.

---

## Option 2: Poetry-Only (`pyproject.toml`)

```toml
[tool.poetry]
name = "wellsfargo_1pii_kpi_automation"
version = "1.1.0"
description = "A Python project for KPI automation"
authors = ["K154844 <mastinder.singh@wellsfargo.com>"]
readme = "README.md"

packages = [
  { include = "kpi_automation" }
]

[[tool.poetry.source]]
name = "KPI_automation"
url = "https://artifactory-intg-b2.wellsfargo.net/artifactory/api"
priority = "primary"

[tool.poetry.dependencies]
python = ">=3.11,<3.12"
jira = "^3.4.0"
psycopg2-binary = "^2.9.5"
pymssql = "^2.3.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

> ✅ Use this if you manage dependencies with `poetry install`, `poetry lock`, and plan to deploy using Poetry pipelines.

---

## Important Notes

- **Do not mix** `setup.cfg` and `tool.poetry` in the same project.
- Maintain **semantic versioning** for easier artifact traceability.
- Always validate syntax (`toml-lint`, `poetry check`, or `python -m build`).

---

Would you like this Confluence content in downloadable `.md` or `.docx` format?







Workplace Experience Platform Services (WXPS) in Wells Fargo – Architecture and Functions

Introduction to WXPS in Enterprise Banking

Workplace Experience Platform Services (WXPS) – also referred to as Workforce Experience & Productivity Solutions (WxPS) – is a Wells Fargo enterprise platform focused on end-user computing and employee technology experience. In essence, WXPS serves as a central system of record (SOR) aggregating data about employees (“team members”), their devices, software, and related IT services ￼. By integrating with various enterprise systems, WXPS provides a unified view of workplace technology assets and enables analytics and reporting to improve employee experience. This platform is a catalyst for optimizing end-user computing, helping IT teams track hardware/software inventory, monitor endpoint health, and deliver insights on employee technology usage and needs.

Key functions of WXPS include:
	•	Data Aggregation: Collecting data from multiple sources (HR, device management, network, security tools, etc.) into a single repository.
	•	Integration: Seamlessly interfacing with enterprise software systems (HR databases, directory services, IT management tools) to ensure data consistency.
	•	Analytics & Reporting: Providing dashboards and reports (e.g. via MicroStrategy and Tableau) for insights into device inventory, software deployment, and employee endpoint analytics.
	•	Enhancing IT Services: Enabling proactive support (by identifying issues like outdated software or security non-compliance) and informed decision-making to improve the workplace tech environment.

Below, we break down the WXPS architecture and components as used in Wells Fargo, explaining each part of the system and how they work together.

High-Level Architecture Overview

At a high level, WXPS is implemented as a data platform that ingests information from numerous enterprise sources, processes and stores this data in structured databases, and then exposes the consolidated data for reporting and analysis. The architecture can be visualized as a pipeline: Data Sources → ETL/Utility Layer → Central Databases (SOR) → Analytics/Reporting Layer.

Figure: Context diagram of WXPS data flow – multiple enterprise data sources feed into the central TMT SOR (Team Member Technology System of Record), which in turn supplies data to enterprise reporting tools like MicroStrategy and Tableau.

In Wells Fargo’s WXPS context diagram (see figure above), the left side lists key data source systems that provide input to WXPS, feeding into a central hub labeled “TMT SOR” (Team Member Technology System of Record). The TMT SOR is the core database/repository in WXPS that consolidates all incoming data. On the right side, the diagram shows enterprise BI tools (MicroStrategy and Tableau) connected to the TMT SOR, indicating that these tools pull data from WXPS for reporting and dashboards. In essence, WXPS acts as a data warehouse for end-user computing data, integrating disparate sources and serving as the “single source of truth” for employee technology assets within the bank.

Let us now examine each part of this system in detail – the data sources feeding WXPS, the utility and integration layer that processes data, the databases that store the consolidated information, and the output/reporting mechanisms – along with how WXPS supports various enterprise functions.

Data Sources Feeding WXPS

WXPS gathers data from a wide range of enterprise systems. These include HR systems, IT asset management tools, device management platforms, directory services, and other infrastructure databases. In Wells Fargo’s environment, some of the main data sources integrated into WXPS (as depicted in the diagrams) are:
	•	PeopleSoft HR Data: The enterprise Human Resources system (PeopleSoft HCM) provides authoritative information on employees (team members). This includes employee IDs, department, manager, location, and role information which is critical for associating devices to specific users or organizational units. PeopleSoft HCM is a comprehensive HR management system that streamlines employee data management and workforce analytics ￼. By pulling HR data, WXPS can tie technical assets to individuals and org structures (e.g., to know which employee or department owns a given laptop).
	•	Active Directory (AD-ENT) and LDAP Directories: Active Directory Enterprise (AD-ENT) is the central directory service for user accounts and network assets. WXPS integrates with AD (likely via LDAP queries) to get data such as user login IDs, group memberships, device objects, and possibly email or NT login associations. This helps in correlating machine data with user accounts and verifying current status (e.g., if a user is active or terminated). Authentication and access controls for WXPS also rely on AD (more on this in security integration).
	•	LAN Central (Network Inventory): Lan Central is an internal system that likely contains network-related data – for example, mappings of devices (or ports) to network locations or IP addresses, and possibly office location information. Feeding this into WXPS allows the platform to determine where a device is connecting from (which site or network segment) and to help track assets geographically or within the network topology. (If an employee moves offices, or connects a device to the network, this data source could update location info in WXPS.)
	•	SCCM / Microsoft Endpoint Config Manager (MSCM): This represents Microsoft’s endpoint management system for Windows PCs (the diagram label “MSCM” likely refers to Microsoft System Center Configuration Manager or a similar tool). SCCM provides detailed hardware and software inventory for Windows endpoints, including installed software titles, versions, OS details, and hardware specifications. It also can supply software deployment status and patch levels. This data is crucial for WXPS’s software inventory function – ensuring the platform knows what software is installed where, and which devices might need updates. SCCM (now part of Microsoft Endpoint Manager) ensures security standards, maintains accurate inventory, and helps deploy software on all Windows devices ￼.
	•	JAMF Pro (Casper Suite): JAMF Pro (formerly called the Casper Suite) is the Apple device management platform used for macOS computers. It provides inventory data and management capabilities for Macs ￼. WXPS pulls data from JAMF to cover all Apple endpoints – this includes Mac hardware details, installed applications on Macs, macOS versions, compliance statuses, etc. By integrating JAMF data, WXPS achieves a full inventory of enterprise devices across both Windows and macOS environments.
	•	Symantec Management/Endpoint: Wells Fargo uses Symantec endpoint management/endpoint protection solutions on client machines. Symantec’s client management suite (formerly Altiris) runs an agent on each computer that automatically gathers inventory data (hardware, OS, installed software) whenever the agent is installed ￼. Additionally, Symantec Endpoint Protection provides security status (antivirus, encryption, etc.). WXPS ingests data from Symantec systems to incorporate security and compliance information about endpoints (e.g., antivirus status, encryption status, etc.). This helps in endpoint analytics, such as identifying machines without updated antivirus or seeing patterns in security incidents.
	•	HPDM (HP Device Manager) & WDM (Wyse Device Manager): These systems manage thin clients or special devices. HP Device Manager is used for HP thin clients, and Wyse Device Manager for Dell Wyse thin clients. They provide inventory and status of these devices. Integrating this data into WXPS allows the platform to account for all endpoint types (not just full PCs, but also thin clients used in branches or call centers).
	•	Controllers DataMart: This likely refers to an internal finance or asset “controllers” data mart. It might contain information about asset ownership, cost centers, depreciation, or financial tracking of equipment. By feeding this into WXPS, the platform can align technical inventory with financial records (e.g., confirming if a device is capitalized, who owns the budget for it, etc.). It could also mean data from “IT controllers” regarding compliance. (In summary, it supplies supplementary data about devices or users from a finance/control perspective.)
	•	PAC2000 / Remedy: PAC2000 is the Wells Fargo internal branding for the BMC Remedy IT Service Management system (as evidenced by employees referring to Remedy/PAC2000 tickets ￼). This system holds IT tickets, incident and request data. WXPS pulling from Remedy means it can incorporate service records – for example, when a device was last serviced, if a user has open IT issues, or records of software requests. This can enrich the WXPS data with lifecycle events (e.g., “Laptop X was requested via PAC2000 ticket #12345 and deployed on date Y”).
	•	Pharos Blueprint (Print Management): Pharos Blueprint is an on-premises print management solution for large enterprises ￼. It tracks printing activity (who printed, how many pages, which printer) and manages print devices. WXPS integrating Blueprint data provides insight into print usage patterns per user or device – an aspect of employee technology experience. For example, it can report how much an employee prints or identify costly printing behaviors, contributing to the overall workplace technology analytics.
	•	Other Data Sources: There may be additional feeds like “HRINV” (possibly HR inventory or a feed of HR changes), and other internal abbreviations (the context diagram had HPDM, WDM, AD-ENT already listed). In summary, any system that holds information about the workplace environment of employees – be it device, software, or user attribute – is a candidate data source for WXPS. By bringing these together, WXPS can present a 360° view of an employee’s technology footprint: their accounts, devices, software, network access, support tickets, and more.

Each of these sources typically provides data to WXPS on a scheduled basis (some daily, some near-real-time, depending on the nature of data and system capabilities). Next, we discuss how WXPS ingests and processes this data.

ETL Utility Layer – Data 

GET .monitoring-es-*/_search
{
  "query": {
    "bool": {
      "filter": [
        { "term": { "type": "node_stats" } },
        { "exists": { "field": "node_stats.breakers" } }
      ]
    }
  },
  "sort": [
    { "timestamp": { "order": "desc" } }
  ],
  "_source": ["timestamp", "node_stats.node_id", "node_stats.breakers"]
}


